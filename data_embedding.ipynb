{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083962d8",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Cada modelo tiene todos los vídeos y la etiqueta en la columna **shoot_zone**,  donde lanzamiento a la derecha es 0, al centro es 1 y a la izquierda  es 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcdd711",
   "metadata": {},
   "source": [
    "| Fichero                      | Modelo          | Comentarios sobre “\\_SUFIJO”                              |\n",
    "| ---------------------------- | --------------- | --------------------------------------------------------- |\n",
    "| **baseline\\_CASIAB.csv**     | Baseline        | Versión “base” (p.ej. GEINet simple) entrenada en CASIA-B |\n",
    "| **baseline\\_OUMVLP.csv**     | Baseline        | Igual que el anterior, pero pre-entrenado en OU-MVLP      |\n",
    "| **gaitgl.csv**               | GaitGL          | GaitGL estándar (dataset por defecto, p.ej. CASIA-B)      |\n",
    "| **gaitgl\\_OUMVLP.csv**       | GaitGL          | Pre-entrenado en OU-MVLP                                  |\n",
    "| **gaitgl\\_GREW\\.csv**        | GaitGL          | Pre-entrenado en GREW                                     |\n",
    "| **gaitgl\\_GREW\\_BNNeck.csv** | GaitGL + BNNeck | Mismo que el anterior, con cuello de batch-norm extra     |\n",
    "| **gaitpart.csv**             | GaitPart        | GaitPart estándar                                         |\n",
    "| **gaitpart\\_OUMVLP.csv**     | GaitPart        | Pre-entrenado en OU-MVLP                                  |\n",
    "| **gaitpart\\_GREW\\.csv**      | GaitPart        | Pre-entrenado en GREW                                     |\n",
    "| **gaitset.csv**              | GaitSet         | GaitSet estándar                                          |\n",
    "| **gaitset\\_OUMVLP.csv**      | GaitSet         | Pre-entrenado en OU-MVLP                                  |\n",
    "| **gaitset\\_GREW\\.csv**       | GaitSet         | Pre-entrenado en GREW                                     |\n",
    "| **gln\\_phase1.csv**          | GLN (fase 1)    | Primer bloque/fase de extracción del modelo “GLN”         |\n",
    "| **gln\\_phase2.csv**          | GLN (fase 2)    | Fase de refinamiento o bloque final del mismo “GLN”       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Directorio con los CSV\n",
    "data_dir = \"Gait_Embeddings_good/\"\n",
    "\n",
    "# 2. Listar sólo los archivos .csv\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "print(\"Archivos encontrados:\", csv_files)\n",
    "\n",
    "# 3. Leer cada CSV en un DataFrame de pandas\n",
    "dfs = {}\n",
    "for fname in csv_files:\n",
    "    path = os.path.join(data_dir, fname)\n",
    "    dfs[fname] = pd.read_csv(path)\n",
    "\n",
    "# 4. Explorar cada DataFrame\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Shape:\", df.shape)                  # filas × columnas\n",
    "    print(\"Columnas:\", df.columns.tolist())    # lista de nombres\n",
    "    print(\"Primeras 5 filas:\")\n",
    "    print(df.head().to_string(index=False))    # muestra las primeras filas\n",
    "\n",
    "    # Opcional: ver tipo de datos y memoria\n",
    "    print(\"\\nInfo:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nDescripción estadística de columnas numéricas:\")\n",
    "    print(df.describe().T)  # transpuesta para leer mejor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f291b9",
   "metadata": {},
   "source": [
    "## Metodología de desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf249d",
   "metadata": {},
   "source": [
    "1️⃣ datos → 2️⃣ preprocesado y normalización → 3️⃣ split → 4️⃣ Dataset/DataLoader (+ collate) → 5️⃣ modelo → 6️⃣ entrenamiento (función de pérdida y optimizador) → 7️⃣ evaluación → 8️⃣ ajuste → 9️⃣ guardado.\n",
    "\n",
    "- Entender y explorar los datos\n",
    "\n",
    "- Inspecciona las columnas, tipos de variables, balance de clases, valores faltantes y rangos.\n",
    "\n",
    "- Visualiza distribuciones y posibles outliers.\n",
    "\n",
    "- Limpieza y preprocesado\n",
    "\n",
    "- Trata valores faltantes (imputación o eliminación).\n",
    "\n",
    "- Normalización / escalado\n",
    "\n",
    "- Aplica Min–Max o Z-score (standarización) para que todas las características queden en un rango controlado y evites que alguna domine el entrenamiento.\n",
    "\n",
    "- En embeddings, a veces se usa L₂-norm para cada vector si quieres que tengan norma unidad.\n",
    "\n",
    "- Dividir en train / validation / test\n",
    "\n",
    "- Reserva al menos un 10–20 % para test “final”.\n",
    "\n",
    "- Dentro del train crea validación (p. ej. 80/20 o K-fold) para ajustar hiperparámetros sin tocar el test.\n",
    "\n",
    "- Definir Dataset y DataLoader para construir batches\n",
    "\n",
    "- Definir el modelo\n",
    "\n",
    "- Elegir función de pérdida y optimizador\n",
    "\n",
    "- Bucle de entrenamiento\n",
    "\n",
    "- Ajuste de hiperparámetros\n",
    "\n",
    "- Guardado y almacenado de los pesos con torch.save(model.state_dict(), 'modelo.pt').\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aece6ba",
   "metadata": {},
   "source": [
    "### Librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0aec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del dispositivo\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c02d79",
   "metadata": {},
   "source": [
    "### Guardado de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5db110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model, path: str):\n",
    "    \"\"\"\n",
    "    Guarda en 'path' únicamente los pesos (state_dict) de `model`.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Modelo guardado en {path}\\n\")\n",
    "\n",
    "\n",
    "def load_model(model_class, path, device=DEVICE, **model_kwargs):\n",
    "    \"\"\"\n",
    "    Carga un modelo de cualquier clase PyTorch definida por el usuario.\n",
    "    \n",
    "    Parámetros:\n",
    "    - model_class: la clase del modelo (MLPClassifier, LSTMClassifier, TransformerClassifier, etc.)\n",
    "    - path:        ruta al archivo .pth con state_dict()\n",
    "    - device:      dispositivo donde cargar el modelo (ej. DEVICE)\n",
    "    - **model_kwargs: argumentos para instanciar la clase de modelo \n",
    "                      (input_dim, hidden_dim, num_layers, ...)\n",
    "    \"\"\"\n",
    "    # Instancia la arquitectura con los kwargs\n",
    "    model = model_class(**model_kwargs).to(device)\n",
    "    # Carga pesos entrenados\n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b5b8c0",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------\n",
    "## Modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95440a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP sencillo con pooling previo\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        input_dim: dimensión D del embedding tras pooling\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 3)  # 3 clases: derecha, centro, izquierda\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, D)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004df432",
   "metadata": {},
   "source": [
    "\n",
    "## Preparación de datos, split y bucle de entrenamiento: MLP\n",
    "Al preparar los datos para el MLP, primero agrupamos todas las filas del CSV que pertenecen a un mismo video_ID en una matriz de tamaño (T, D), donde T es el número de frames de ese vídeo y D las 256 características por frame. A continuación aplicamos mean‐pooling o max‐pooling sobre el eje temporal T, colapsando cada matriz a un único vector de dimensión (D,) que resume toda la aproximación del jugador al penalti. Ese conjunto de vectores —uno por vídeo— se divide de forma estratificada en train y test, de modo que en el entrenamiento el DataLoader extrae batches de tamaño fijo (por ejemplo 32) con esos vectores y sus etiquetas, y así el MLP aprende a clasificar la dirección del lanzamiento usando esos resúmenes globales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bfa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def prepare_mlp_data(df, pooling, norm, test_size=0.2):\n",
    "    # 1) Pooling temporal\n",
    "    feat_cols = [c for c in df.columns if c.startswith('feat_')]\n",
    "    seqs, labs = [], []\n",
    "    for vid, grp in df.groupby('video_ID'):\n",
    "        arr = grp[feat_cols].values.astype(np.float32)  # (T, D)\n",
    "        vec = arr.mean(axis=0) if pooling=='mean' else arr.max(axis=0)\n",
    "        seqs.append(vec)\n",
    "        labs.append(int(grp['shoot_zone'].iloc[0]))\n",
    "    X = np.stack(seqs)  # (N, D)\n",
    "    y = np.array(labs, dtype=np.int64)\n",
    "\n",
    "    # 2) Normalización\n",
    "    if norm == 'minmax':\n",
    "        X = MinMaxScaler().fit_transform(X)\n",
    "    elif norm == 'L2':  # 'l2'\n",
    "        X = normalize(X, norm='l2')\n",
    "\n",
    "    # 3) Split estratificado\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
    "\n",
    "    # 4) TensorDatasets\n",
    "    tr_ds = TensorDataset(torch.from_numpy(Xtr).float(),\n",
    "                          torch.from_numpy(ytr))\n",
    "    te_ds = TensorDataset(torch.from_numpy(Xte).float(),\n",
    "                          torch.from_numpy(yte))\n",
    "    return tr_ds, te_ds\n",
    "\n",
    "def run_training(model, train_loader, test_loader, epochs=20, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4) # REVISAR\n",
    "    loss_fn   = nn.CrossEntropyLoss()\n",
    "    history   = {'train_loss':[], 'test_acc':[], 'test_f1':[]}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # entrenamiento\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "        history['train_loss'].append(total_loss / len(train_loader.dataset))\n",
    "\n",
    "        # evaluación\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                preds = model(xb).argmax(dim=1).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(yb.numpy())\n",
    "        all_preds  = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1  = f1_score(all_labels, all_preds, average='macro')\n",
    "        history['test_acc'].append(acc)\n",
    "        history['test_f1'].append(f1)\n",
    "\n",
    "        print(f\"Ep{ep:02d} | loss {history['train_loss'][-1]:.4f} \"\n",
    "              f\"| acc {acc:.4f} | f1_macro {f1:.4f}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4b152",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d20515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = \"Gait_Embeddings_good\"\n",
    "POOLS    = ['mean', 'max']\n",
    "NORMS    = ['minmax', 'L2']\n",
    "EPOCHS   = 1000\n",
    "BATCH    = 64\n",
    "LOAD_MODEL = False  # Si True, carga un modelo preentrenado en lugar de entrenar uno nuevo\n",
    "\n",
    "PATH = \"saved_models/MLP/\"\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for fname in os.listdir(DATA_DIR):\n",
    "    if not fname.endswith('.csv'):\n",
    "        continue\n",
    "    print(f\"\\n--- Entrenando MLP con {fname} ---\")\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, fname))\n",
    "\n",
    "    for pool in POOLS:\n",
    "        for norm in NORMS:\n",
    "            tr_ds, te_ds = prepare_mlp_data(df, pooling=pool, norm=norm, test_size=0.2)\n",
    "            tr_loader = DataLoader(tr_ds, batch_size=BATCH, shuffle=True)\n",
    "            te_loader = DataLoader(te_ds, batch_size=BATCH)\n",
    "\n",
    "            model = MLPClassifier(tr_ds[0][0].shape[0]).to(DEVICE) \n",
    "            \n",
    "            # Si LOAD_MODEL es True, intenta cargar un modelo preentrenado\n",
    "            if LOAD_MODEL == True:\n",
    "                model_path = os.path.join(PATH, f\"mlp_{pool}_{norm}_{fname.replace('.csv', '')}.pth\")\n",
    "                if os.path.exists(model_path):\n",
    "                    print(f\"Cargando modelo preentrenado desde {model_path}\")\n",
    "                    model = load_model(MLPClassifier, model_path, input_dim=tr_ds[0][0].shape[0])\n",
    "            \n",
    "            history = run_training(model, tr_loader, te_loader, epochs=EPOCHS, lr=1e-3)\n",
    "\n",
    "            results.append({\n",
    "                'extractor':     fname,\n",
    "                'model':         'MLP',\n",
    "                'epochs':        EPOCHS,\n",
    "                'batch_size':    BATCH,\n",
    "                'pooling':       pool,\n",
    "                'normalization': norm,\n",
    "                'accuracy':      round(history['test_acc'][-1], 5),\n",
    "                'f1_macro':      round(history['test_f1'][-1], 5)\n",
    "            })\n",
    "            \n",
    "            # Guardar modelo\n",
    "            model_path = os.path.join(PATH, f\"mlp_{pool}_{norm}_{fname.replace('.csv', '')}.pth\")\n",
    "            save_model(model, model_path)\n",
    "\n",
    "\n",
    "# Guardar resultados finales\n",
    "df_res = pd.DataFrame(results)\n",
    "df_res.to_csv(f\"results/mlp_penalty_results_{BATCH}.csv\", index=False)\n",
    "print(\"\\n✅ Resultados guardados en mlp_penalty_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc74d1",
   "metadata": {},
   "source": [
    "## Matriz de Confusión y Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e41a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_confusion_matrix(model, dataloader, device, class_names):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Adaptar según tu DataLoader (2 ó 3 elementos)\n",
    "            if len(batch) == 3:\n",
    "                xb, lengths, yb = batch\n",
    "                xb, lengths = xb.to(device), lengths.to(device)\n",
    "                logits = model(xb, lengths)\n",
    "            else:\n",
    "                xb, yb = batch\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb)\n",
    "\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(yb.numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Dibujar\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Matriz de Confusión\")\n",
    "    plt.colorbar()\n",
    "    ticks = np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45)\n",
    "    plt.yticks(ticks, class_names)\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel(\"Etiqueta real\")\n",
    "    plt.xlabel(\"Etiqueta predicha\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return cm\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_multiclass_roc(model, dataloader, device, class_names):\n",
    "    model.eval()\n",
    "    y_true, y_score = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if len(batch) == 3:\n",
    "                xb, lengths, yb = batch\n",
    "                xb, lengths = xb.to(device), lengths.to(device)\n",
    "                logits = model(xb, lengths)\n",
    "            else:\n",
    "                xb, yb = batch\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb)\n",
    "\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            y_score.append(probs)\n",
    "            y_true.append(yb.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_score = np.concatenate(y_score)\n",
    "    n_classes = y_score.shape[1]\n",
    "    y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC={roc_auc:.2f})\")\n",
    "\n",
    "    plt.plot([0,1], [0,1], 'k--', label=\"Azar\")\n",
    "    plt.xlabel(\"Tasa Falsos Positivos\")\n",
    "    plt.ylabel(\"Tasa Verdaderos Positivos\")\n",
    "    plt.title(\"ROC Multi-clase (One-vs-Rest)\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d953ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumiendo que ya tienes un modelo entrenado y un DataLoader de test\n",
    "class_names = ['derecha', 'centro', 'izquierda']\n",
    "evaluate_multiclass_roc(model, te_loader, DEVICE, class_names)\n",
    "cm = evaluate_confusion_matrix(model, te_loader, DEVICE, class_names)\n",
    "\n",
    "# Numero de aciertos y errores\n",
    "print(\"\\n=== Resultados de la matriz de confusión para test ===\")\n",
    "print(\"Aciertos (diagonal):\", np.diag(cm).sum())\n",
    "print(\"Errores (fuera de la diagonal):\", cm.sum() - np.diag(cm).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552f477",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Modelo LSTM y BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,      # 256 features del CSV\n",
    "        hidden_dim,     # Dimensión oculta de la LSTM \n",
    "        num_layers,     # Número de capas LSTM \n",
    "        bidirectional,  # Si usar BiLSTM\n",
    "        dropout        # Dropout entre capas y antes de clasificación\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Capas LSTM apiladas con dropout entre ellas\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0  # Dropout solo si hay múltiples capas\n",
    "        )\n",
    "        \n",
    "        # 2. Dimensiones de salida\n",
    "        self.lstm_out_dim = hidden_dim * (2 if bidirectional else 1)  # Dimensión de salida LSTM\n",
    "        self.hidden_dim = self.lstm_out_dim // 2 if bidirectional else self.lstm_out_dim\n",
    "\n",
    "        # 3. Capa densa final\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.lstm_out_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, 3)  # 3 clases: derecha, centro, izquierda\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # 1. Empaquetar secuencias variables\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), \n",
    "                                    batch_first=True, \n",
    "                                    enforce_sorted=False)\n",
    "\n",
    "        # 2. Procesar con LSTM\n",
    "        # Shape de h_n: (num_layers * 2, batch_size, hidden_dim) el caso de BiLSTM\n",
    "        # el num_layers * 2 es porque hay una capa forward y otra backward de ahí el 2\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        \n",
    "        # 3. Obtener estado final (último estado de la última capa)\n",
    "        if self.lstm.bidirectional:\n",
    "            h_forward = h_n[-2]  # última capa forward\n",
    "            h_backward = h_n[-1]  # última capa backward\n",
    "            h_final = torch.cat([h_forward, h_backward], dim=1)\n",
    "        else:\n",
    "            h_final = h_n[-1]\n",
    "\n",
    "        # 4. Pasar por la capa densa final\n",
    "        return self.fc(h_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc2d0d",
   "metadata": {},
   "source": [
    "\n",
    "## Preparación de datos, split y bucle de entrenamiento: LSTM\n",
    "\n",
    "**Normalización**: antes de la partición train/test se aplican dos esquemas alternativos de escalado MinMax (entrenando un MinMaxScaler sobre todos los frames y transformando cada secuencia) o L2-norm (normalizando cada frame por su norma L2) para unificar la escala de las características. Función: **`prepare_seq_data`**\n",
    "\n",
    "**DataLoader con función `collate_sequences`**: cada lote de secuencias de longitud variable se rellena (pad) al tamaño de la más larga y se devuelve un tensor de longitudes, lo que permite a la LSTM procesar correctamente secuencias de distinta longitud en un mismo batch.\n",
    "\n",
    "**Optimización con Adam, weight decay y grad-clip**: se entrena con Adam, añadiendo regularización L2 (weight_decay) y aplicando clip_grad_norm_ tras el backward para limitar la magnitud de los gradientes y prevenir explosiones que se manifiestan como picos abruptos en la train_loss. Función: **`run_training_lstm`**\n",
    "\n",
    "**Early stopping**: tras 300 épocas sin una reducción significativa de la train_loss (más allá de un umbral min_delta), el entrenamiento se detiene para evitar sobreajuste y ahorro de recursos computacionales. `''`\n",
    "\n",
    "**Seguimiento del mejor F1 con “warm-up”**: el cómputo de best_f1_score comienza únicamente a partir de la época 100. Esto impide registrar picos de F1 obtenidos por azar en fases iniciales, cuando el train_loss sigue siendo alto y el modelo aún no ha aprendido de forma estable. Establecer este mínimo de 100 épocas asegura que las mejoras de F1 reflejen un aprendizaje consolidado y verdaderamente generalizable. `''`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ec41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_sequences(batch):\n",
    "    \"\"\"\n",
    "    Recibe una lista de (tensor_seq, label).\n",
    "    Devuelve:\n",
    "      - padded: tensor (B, T_max, D)\n",
    "      - lengths: tensor (B,)\n",
    "      - labels: tensor (B,)\n",
    "    \"\"\"\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([s.size(0) for s in seqs], dtype=torch.long)\n",
    "    padded = pad_sequence(seqs, batch_first=True)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded, lengths, labels\n",
    "\n",
    "def prepare_seq_data(df, norm, test_size):\n",
    "    \"\"\"\n",
    "    df: DataFrame con columnas feat_0…feat_D-1, video_ID y shoot_zone\n",
    "    norm: 'minmax' o 'l2'\n",
    "    Devuelve dos listas de muestras (tensor_seq, label) para train y test.\n",
    "    \"\"\"\n",
    "    # 1) Extraer todas las secuencias y etiquetas\n",
    "    feat_cols = [c for c in df.columns if c.startswith('feat_')]\n",
    "    seqs, labs = [], []\n",
    "    for vid, grp in df.groupby('video_ID'):\n",
    "        arr = grp[feat_cols].values.astype(np.float32)  # (T, D)\n",
    "        seqs.append(arr)\n",
    "        labs.append(int(grp['shoot_zone'].iloc[0]))\n",
    "\n",
    "    # 2) Normalizar\n",
    "    if norm == 'minmax':\n",
    "        all_frames = np.vstack(seqs)\n",
    "        scaler = MinMaxScaler().fit(all_frames)\n",
    "        seqs = [scaler.transform(s) for s in seqs]\n",
    "    elif norm == 'L2':  # L2-norm por frame\n",
    "        seqs = [normalize(s, norm='l2', axis=1) for s in seqs]\n",
    "\n",
    "    # 3) Split estratificado\n",
    "    idx = list(range(len(seqs)))\n",
    "    idx_tr, idx_te = train_test_split(idx, test_size=test_size, stratify=labs, random_state=16)\n",
    "\n",
    "    # 4) Convertir a listas de tuplas (tensor_seq, label)\n",
    "    train_list = [(torch.from_numpy(seqs[i]), labs[i]) for i in idx_tr]\n",
    "    test_list  = [(torch.from_numpy(seqs[i]), labs[i]) for i in idx_te]\n",
    "\n",
    "\n",
    "    return train_list, test_list\n",
    "\n",
    "\n",
    "def run_training_lstm(model, train_loader, test_loader, epochs, lr, wd, min_epochs):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento específica para el modelo LSTM\n",
    "    Args:\n",
    "        model: Instancia de LSTMClassifier\n",
    "        train_loader: DataLoader con datos de entrenamiento (incluye lengths)\n",
    "        test_loader: DataLoader con datos de test (incluye lengths)\n",
    "        epochs: Número de épocas de entrenamiento\n",
    "        lr: Learning rate para el optimizador\n",
    "    Returns:\n",
    "        history: Diccionario con métricas de entrenamiento\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    history = {'train_loss': [],'test_loss':[], 'test_acc': [], 'test_f1': []}\n",
    "\n",
    "    patience = 50  # Paciencia para early stopping\n",
    "    min_delta = 0.0001  # Mínima mejora para considerar que hay progreso\n",
    "    best_loss = float('inf')  # Mejor pérdida inicial\n",
    "    epochs_no_improve = 0  # Contador de épocas sin mejora\n",
    "\n",
    "    best_f1_score = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        # --- TRAIN ---\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, lengths, yb in train_loader:\n",
    "            # Mover datos a GPU/CPU\n",
    "            xb = xb.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            yb = yb.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb, lengths)\n",
    "            loss = loss_fn(out, yb)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Clipping de gradientes para evitar explosiones\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(avg_loss)\n",
    "\n",
    "        # --- TEST ---\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        #test_sum = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb, lengths, yb in test_loader:\n",
    "                xb = xb.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                \n",
    "                # Predicción\n",
    "                outputs = model(xb, lengths)\n",
    "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                \n",
    "                #test_sum += loss_fn(outputs, yb).item() * yb.size(0)\n",
    "                \n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(yb.numpy())\n",
    "\n",
    "        # Calcular métricas\n",
    "        #avg_test_loss = test_sum / len(test_loader.dataset)\n",
    "        #history['test_loss'].append(avg_test_loss)\n",
    "\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        history['test_acc'].append(acc)\n",
    "        history['test_f1'].append(f1)\n",
    "\n",
    "        # Imprimir progreso\n",
    "        print(f\"Época {ep:02d}/{epochs} | \"\n",
    "              f\"train_loss {avg_loss:.4f} | \"\n",
    "              f\"accuracy {acc:.4f} | \"\n",
    "              f\"f1_macro {f1:.4f}\")\n",
    "        \n",
    "        # trackear best F1 solo tras un mínimo de épocas\n",
    "        if ep > min_epochs and f1 > best_f1_score + 1e-5:\n",
    "            best_f1_score = f1\n",
    "            best_epoch = ep\n",
    "\n",
    "        # — Early stopping basado en train_loss — o se puede hacer para test_loss\n",
    "        if ep > min_epochs:\n",
    "            if avg_loss < best_loss - min_delta:\n",
    "                best_loss = avg_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"\\nEarly stopping tras {epochs_no_improve} épocas \"\n",
    "                          f\"sin mejorar el train_loss (delta<{min_delta}).\")\n",
    "                    break\n",
    "\n",
    "    return history, ep, best_f1_score, best_epoch\n",
    "\n",
    "# REVISAR: en vez de devolver ep y best_f1_score, devolver un diccionario con todo dentro de history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268058bf",
   "metadata": {},
   "source": [
    "## Optimización de hiperparámetros\n",
    "\n",
    "Se emplea StratifiedKFold sobre los IDs de vídeo (video_ID), no sobre los fotogramas individuales, para crear 5 particiones que mantienen la proporción de clases en cada fold. Para cada partición, se extraen dos listas de vídeos: una de entrenamiento y otra de validación, y se filtra el DataFrame original para incluir únicamente las secuencias completas de esos vídeos. Cada secuencia se normaliza (MinMax o L2) y se empaqueta en un DataLoader con collate_sequences, garantizando que ningún vídeo se mezcle entre train/val y preservando su continuidad temporal. Así se evita la fuga de información entre folds y se evalúa el modelo sobre vídeos inéditos en cada ronda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuración global\n",
    "DATA_DIR = \"Gait_Embeddings_good\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def objective(trial, df):\n",
    "    \"\"\"Función objetivo para Optuna\"\"\"\n",
    "    config = {\n",
    "        'hidden_dim': trial.suggest_categorical(\"hidden_dim\", [64, 128, 256, 512]),\n",
    "        'num_layers': trial.suggest_int(\"num_layers\", 1, 3),\n",
    "        'bidirectional': trial.suggest_categorical(\"bidirectional\", [False, True]),\n",
    "        'dropout': trial.suggest_categorical(\"dropout\", [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "        'lr': trial.suggest_categorical(\"lr\", [1e-5, 1e-4, 1e-3]),\n",
    "        'batch_size': trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "        'norm': trial.suggest_categorical(\"norm\", [\"minmax\", \"L2\"])\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Trial {trial.number}\")\n",
    "    print(f\"Configuración: {config}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    # Preparar datos para K-Fold\n",
    "    video_ids = list(df.groupby('video_ID').groups.keys())\n",
    "    labels = [int(df[df['video_ID']==vid]['shoot_zone'].iloc[0]) for vid in video_ids]\n",
    "\n",
    "    # K-Fold Cross Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=16)\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(video_ids, labels)):\n",
    "        print(f\"\\nFold {fold+1}/5\")\n",
    "        \n",
    "        # Separar datos de train/val\n",
    "        train_ids = [video_ids[i] for i in train_idx]\n",
    "        val_ids = [video_ids[i] for i in val_idx]\n",
    "        \n",
    "        df_train = df[df['video_ID'].isin(train_ids)]\n",
    "        df_val = df[df['video_ID'].isin(val_ids)]\n",
    "        \n",
    "        # Preparar datos\n",
    "        feat_cols = [c for c in df_train.columns if c.startswith('feat_')]\n",
    "        \n",
    "        # Train data\n",
    "        train_seqs = []\n",
    "        train_labs = []\n",
    "        for vid, grp in df_train.groupby('video_ID'):\n",
    "            arr = grp[feat_cols].values.astype(np.float32)\n",
    "            if config['norm'] == 'minmax':\n",
    "                arr = MinMaxScaler().fit_transform(arr)\n",
    "            elif config['norm'] == 'L2':  \n",
    "                arr = normalize(arr, norm='l2', axis=1)\n",
    "            train_seqs.append(torch.from_numpy(arr))\n",
    "            train_labs.append(int(grp['shoot_zone'].iloc[0]))\n",
    "        \n",
    "        # Val data\n",
    "        val_seqs = []\n",
    "        val_labs = []\n",
    "        for vid, grp in df_val.groupby('video_ID'):\n",
    "            arr = grp[feat_cols].values.astype(np.float32)\n",
    "            if config['norm'] == 'minmax':\n",
    "                arr = MinMaxScaler().fit_transform(arr)\n",
    "            else:  # L2\n",
    "                arr = normalize(arr, norm='l2', axis=1)\n",
    "            val_seqs.append(torch.from_numpy(arr))\n",
    "            val_labs.append(int(grp['shoot_zone'].iloc[0]))\n",
    "\n",
    "        # Crear datasets y dataloaders\n",
    "        train_data = list(zip(train_seqs, train_labs))\n",
    "        val_data = list(zip(val_seqs, val_labs))\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_sequences)\n",
    "        val_loader = DataLoader(val_data, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_sequences)\n",
    "\n",
    "        # Crear y entrenar modelo\n",
    "        model = LSTMClassifier(\n",
    "            input_dim=df.filter(like='feat_').shape[1],\n",
    "            hidden_dim=config['hidden_dim'],\n",
    "            num_layers=config['num_layers'],\n",
    "            bidirectional=config['bidirectional'],\n",
    "            dropout=config['dropout']\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Entrenar\n",
    "        history, _, _, _ = run_training_lstm(\n",
    "            model=model,\n",
    "            train_loader=train_loader, \n",
    "            test_loader=val_loader,\n",
    "            epochs=200,  \n",
    "            lr=config['lr'],\n",
    "            wd=0, # No usamos weight decay aquí\n",
    "            min_epochs=100 # para early stopping\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Guardar mejor F1 score del fold\n",
    "        fold_scores.append(max(history['test_f1']))\n",
    "\n",
    "    mean_f1 = np.mean(fold_scores)\n",
    "    print(f\"\\nConfiguración: {config}\")\n",
    "    print(f\"F1-score medio: {mean_f1:.4f}\")\n",
    "    \n",
    "    return mean_f1\n",
    "\n",
    "\n",
    "\n",
    "def optimize_embeddings():\n",
    "    \"\"\"Ejecuta optimización para cada embedding base\"\"\"\n",
    "    \n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results = {}\n",
    "    \n",
    "    for fname in os.listdir(DATA_DIR):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Optimizando {fname}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Cargar datos\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, fname))\n",
    "        \n",
    "        # Crear y ejecutar estudio\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, df),\n",
    "            n_trials=20,  # 30 trials por embedding\n",
    "            n_jobs=10,     # Paralelización\n",
    "            timeout=3600  # Timeout de 1 hora por embedding\n",
    "        )\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results[fname] = {\n",
    "            'best_params': study.best_params,\n",
    "            'best_f1': study.best_value\n",
    "        }\n",
    "        \n",
    "        # Guardar trials en CSV\n",
    "        study_df = study.trials_dataframe()\n",
    "        study_df.to_csv(f\"results/LSTM_hyperparams_{fname}.csv\", index=False)\n",
    "        \n",
    "        print(f\"\\nMejores parámetros para {fname}:\")\n",
    "        print(f\"F1-score: {study.best_value:.4f}\")\n",
    "        print(\"Configuración:\", study.best_params)\n",
    "        \n",
    "    # Guardar resumen final\n",
    "    final_results = []\n",
    "    for embedding, res in results.items():\n",
    "        row = {\n",
    "            'embedding': embedding,\n",
    "            'best_f1': res['best_f1'],\n",
    "            **res['best_params']\n",
    "        }\n",
    "        final_results.append(row)\n",
    "    \n",
    "    df_final = pd.DataFrame(final_results)\n",
    "    df_final.to_csv(\"results/best_parameters/best_params_LSTM.csv\", index=False)\n",
    "    print(\"\\nResumen final guardado en results/best/parameters/best_params_LSTM.csv\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e531ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna\n",
    "results = optimize_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a897a0",
   "metadata": {},
   "source": [
    "## Entrenamiento PROBAR VARIAS CONFIGURACIONES \n",
    "Configuración de hiperparámetros a partir de los resultados obtenidos mediante Optuna.\n",
    "- Mencionar que las métricas que se añaden a los resultados obtenidos son las siguientes:\n",
    "- f1-score..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24265cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = \"Gait_Embeddings_good\"\n",
    "EPOCHS = 1000\n",
    "BATCH = 64\n",
    "LR = 1e-3\n",
    "WD = 0.0\n",
    "NORM = 'L2'\n",
    "F1_THRESHOLD = 0.5  # Umbral mínimo de F1 para considerar un modelo válido\n",
    "LOAD_MODEL = False  # Si True, carga un modelo preentrenado en lugar de entrenar uno nuevo\n",
    "\n",
    "\n",
    "# Hiperparámetros de LSTM\n",
    "num_layers    = 1\n",
    "hidden_dim    = 128\n",
    "bidirectional = True\n",
    "dropout       = 0.2\n",
    "\n",
    "\n",
    "results_lstm = []\n",
    "\n",
    "\n",
    "for fname in os.listdir(DATA_DIR):\n",
    "    if not fname.endswith('.csv'):\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n--- Entrenando LSTM con {fname} ---\")\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, fname))\n",
    "    \n",
    "    # Preparar datos\n",
    "    train_list, test_list = prepare_seq_data(df, norm=NORM, test_size=0.2)\n",
    "    tr_loader = DataLoader(train_list, batch_size=BATCH, shuffle=True, collate_fn=collate_sequences)\n",
    "    te_loader = DataLoader(test_list, batch_size=BATCH, shuffle=False, collate_fn=collate_sequences)\n",
    "\n",
    "    # Crear y entrenar modelo\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=df.filter(like='feat_').shape[1],\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        bidirectional=bidirectional,\n",
    "        dropout=dropout\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # REVISAR EL MEOTDO DE CARGA DEL MODELO NO VA BIEN\n",
    "    # Si LOAD_MODEL es True, intenta cargar un modelo preentrenado\n",
    "    if LOAD_MODEL:\n",
    "        model_path = os.path.join(\"saved_models/LSTM\", f\"lstm_{NORM}_hidden{hidden_dim}_layers{num_layers}_batch{BATCH}_LR{LR}_dropout{dropout}_BiLSTM_{bidirectional}_{fname.replace('.csv', '')}.pth\")\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Cargando modelo preentrenado desde {model_path}\")\n",
    "            model = load_model(LSTMClassifier, model_path, input_dim=df.filter(like='feat_').shape[1])\n",
    "\n",
    "    history, ep, best_f1, best_epoch = run_training_lstm(model, tr_loader, te_loader, epochs=EPOCHS, lr=LR, wd=WD, min_epochs=100) \n",
    "\n",
    "\n",
    "    # Guardar resultados\n",
    "    results_lstm.append({\n",
    "        'extractor': fname,\n",
    "        'model': 'LSTM',\n",
    "        'epochs': ep,\n",
    "        'batch_size': BATCH,\n",
    "        'normalization': NORM,\n",
    "        'num_layers': num_layers,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'bidirectional': bidirectional,\n",
    "        'dropout': dropout,\n",
    "        'lr': LR,\n",
    "        'train_loss': round(history['train_loss'][-1], 5),\n",
    "        'accuracy': round(history['test_acc'][-1], 5),\n",
    "        'f1_macro': round(history['test_f1'][-1], 5),\n",
    "        \n",
    "        'best_f1_score': round(best_f1, 5),\n",
    "        'best_epoch': best_epoch\n",
    "\n",
    "    })\n",
    "\n",
    "    if best_f1 < F1_THRESHOLD:\n",
    "        print(f\"\\nModelo con F1 {best_f1:.4f} por debajo del umbral {F1_THRESHOLD}. No se guardará.\")\n",
    "        continue\n",
    "\n",
    "    # Guardar modelo\n",
    "    model_path = os.path.join(\"saved_models/LSTM\", f\"lstm_{NORM}_hidden{hidden_dim}_layers{num_layers}_batch{BATCH}_LR{LR}_dropout{dropout}_BiLSTM_{bidirectional}_{fname.replace('.csv', '')}.pth\")\n",
    "    save_model(model, model_path)\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results_lstm)\n",
    "results_path = \"results/lstm_results.csv\"\n",
    "\n",
    "# Si el archivo existe, añadir los nuevos resultados al final\n",
    "if os.path.exists(results_path):\n",
    "    df_existing = pd.read_csv(results_path)\n",
    "    df_combined = pd.concat([df_existing, df_results], ignore_index=True)\n",
    "    df_combined.to_csv(results_path, index=False)\n",
    "    print(f\"\\n Resultados LSTM añadidos a {results_path}\")\n",
    "else:\n",
    "    # Si no existe, crear nuevo archivo\n",
    "    df_results.to_csv(results_path, index=False)\n",
    "    print(f\"\\n Nuevo archivo de resultados LSTM creado en {results_path}\")\n",
    "\n",
    "# Mostrar todos los resultados\n",
    "print(\"\\nResumen de todos los resultados:\")\n",
    "print(pd.read_csv(results_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef3c03",
   "metadata": {},
   "source": [
    "## Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cde770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumiendo que ya tienes un modelo entrenado y un DataLoader de test\n",
    "class_names = ['derecha', 'centro', 'izquierda']\n",
    "evaluate_multiclass_roc(model, te_loader, DEVICE, class_names)\n",
    "cm = evaluate_confusion_matrix(model, te_loader, DEVICE, class_names)\n",
    "\n",
    "# Numero de aciertos y errores\n",
    "print(\"\\n=== Resultados de la matriz de confusión para test ===\")\n",
    "print(\"Aciertos (diagonal):\", np.diag(cm).sum())\n",
    "print(\"Errores (fuera de la diagonal):\", cm.sum() - np.diag(cm).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc3f4a2",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Modelo Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4bb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Transformer con atención temporal\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        model_dim=256,\n",
    "        n_heads=4,\n",
    "        num_layers=2,\n",
    "        ff_dim=512,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_dim: dimensión D del embedding sin pooling (secuencia)\n",
    "        model_dim: dimensión interna del transformer\n",
    "        n_heads: número de cabezas de atención\n",
    "        num_layers: número de capas Encoder\n",
    "        ff_dim: dimensión del feed-forward\n",
    "        dropout: dropout en capas Encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Proyección de la dimensión de entrada al espacio model_dim\n",
    "        self.token_proj = nn.Linear(input_dim, model_dim)\n",
    "        # Capa TransformerEncoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Clasificador al token de posición 0 (CLS) o media de salidas\n",
    "        self.classifier = nn.Linear(model_dim, 3)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        \"\"\"\n",
    "        x: FloatTensor (B, T, D)\n",
    "        lengths: LongTensor (B,) opcional para masking\n",
    "        \"\"\"\n",
    "        # x → proyección\n",
    "        x = self.token_proj(x)  # (B, T, model_dim)\n",
    "\n",
    "        # Generar máscara de padding si lengths dado\n",
    "        if lengths is not None:\n",
    "            max_len = x.size(1)\n",
    "            mask = torch.arange(max_len, device=lengths.device) \\\n",
    "                   .unsqueeze(0) >= lengths.unsqueeze(1)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        # TransformerEncoder\n",
    "        out = self.transformer(x, src_key_padding_mask=mask)  # (B, T, model_dim)\n",
    "\n",
    "        # Agregado temporal: tomamos token 0 como representativo\n",
    "        cls_token = out[:, 0, :]  # (B, model_dim)\n",
    "        return self.classifier(cls_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c355e",
   "metadata": {},
   "source": [
    "\n",
    "## Preparación de datos, split y bucle de entrenamiento: Transformer\n",
    "Con atención atemporal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 2: Collate function y preparación de datos para Transformer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_sequences(batch):\n",
    "    \"\"\"\n",
    "    Recibe una lista de tuplas (tensor_seq, label).\n",
    "    Devuelve:\n",
    "      - padded: FloatTensor (B, T_max, D)\n",
    "      - lengths: LongTensor (B,)\n",
    "      - labels: LongTensor (B,)\n",
    "    \"\"\"\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([s.size(0) for s in seqs], dtype=torch.long)\n",
    "    padded  = pad_sequence(seqs, batch_first=True)\n",
    "    labels  = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded, lengths, labels\n",
    "\n",
    "def prepare_transformer_data(df, norm='minmax', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Construye listas de muestras para train y test:\n",
    "      - norm: 'minmax' o 'l2'\n",
    "    Cada muestra es (tensor_seq, label).\n",
    "    \"\"\"\n",
    "    feat_cols = [c for c in df.columns if c.startswith('feat_')]\n",
    "    seqs, labs = [], []\n",
    "    for vid, grp in df.groupby('video_ID'):\n",
    "        arr = grp[feat_cols].values.astype(np.float32)  # (T, D)\n",
    "        seqs.append(arr)\n",
    "        labs.append(int(grp['shoot_zone'].iloc[0]))\n",
    "    # Normalización\n",
    "    if norm == 'minmax':\n",
    "        all_frames = np.vstack(seqs)  # (sum_T, D)\n",
    "        scaler = MinMaxScaler().fit(all_frames)\n",
    "        seqs = [scaler.transform(s) for s in seqs]\n",
    "    else:  # 'l2'\n",
    "        seqs = [s / np.linalg.norm(s, axis=1, keepdims=True) for s in seqs]\n",
    "    # Split estratificado\n",
    "    idx = list(range(len(seqs)))\n",
    "    idx_tr, idx_te = train_test_split(idx,\n",
    "                                      test_size=test_size,\n",
    "                                      stratify=labs,\n",
    "                                      random_state=42)\n",
    "    # Convertir a lista de tuplas (tensor_seq, label)\n",
    "    train_list = [(torch.from_numpy(seqs[i]), labs[i]) for i in idx_tr]\n",
    "    test_list  = [(torch.from_numpy(seqs[i]), labs[i]) for i in idx_te]\n",
    "    return train_list, test_list\n",
    "\n",
    "\n",
    "def run_training_transformer(model, train_loader, test_loader, epochs=20, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento específica para el modelo Transformer\n",
    "    Args:\n",
    "        model: Instancia de TransformerClassifier\n",
    "        train_loader: DataLoader con datos de entrenamiento \n",
    "        test_loader: DataLoader con datos de test\n",
    "        epochs: Número de épocas de entrenamiento\n",
    "        lr: Learning rate para el optimizador\n",
    "    Returns:\n",
    "        history: Diccionario con métricas de entrenamiento\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    history = {'train_loss': [], 'test_acc': [], 'test_f1': []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        # --- Fase de entrenamiento ---\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, lengths, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            yb = yb.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb, lengths)\n",
    "            loss = loss_fn(out, yb)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(avg_loss)\n",
    "\n",
    "        # --- Fase de evaluación ---\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb, lengths, yb in test_loader:\n",
    "                xb = xb.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                outputs = model(xb, lengths)\n",
    "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(yb.numpy())\n",
    "\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        history['test_acc'].append(acc)\n",
    "        history['test_f1'].append(f1)\n",
    "\n",
    "        print(f\"Época {ep:02d}/{epochs} | \"\n",
    "                f\"loss {avg_loss:.4f} | \"\n",
    "                f\"acc {acc:.4f} | \"\n",
    "                f\"f1_macro {f1:.4f}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e5359",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fe94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuración\n",
    "\n",
    "DATA_DIR = \"Gait_Embeddings_good\"\n",
    "NORMS = ['minmax', 'l2']\n",
    "EPOCHS = 100  # Aumentado para mejor convergencia\n",
    "BATCH_SIZE = 32  # Tamaño de batch para entrenamiento\n",
    "LR = 1e-3\n",
    "MODEL_DIM = 256  # Dimensión del transformer\n",
    "N_HEADS = 8  # Número de cabezas de atención\n",
    "N_LAYERS = 2  # Capas del encoder\n",
    "\n",
    "results_transformer = []\n",
    "\n",
    "for fname in os.listdir(DATA_DIR):\n",
    "    if not fname.endswith('.csv'):\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n--- Entrenando Transformer con {fname} ---\")\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, fname))\n",
    "    \n",
    "    for norm in NORMS:\n",
    "        # Preparar datos\n",
    "        train_list, test_list = prepare_transformer_data(df, norm=norm)\n",
    "        tr_loader = DataLoader(train_list, \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             shuffle=True, \n",
    "                             collate_fn=collate_sequences)\n",
    "        te_loader = DataLoader(test_list, \n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=collate_sequences)\n",
    "\n",
    "        # Crear y entrenar modelo\n",
    "        model = TransformerClassifier(\n",
    "            input_dim=df.filter(like='feat_').shape[1],\n",
    "            model_dim=MODEL_DIM,\n",
    "            n_heads=N_HEADS,\n",
    "            num_layers=N_LAYERS,\n",
    "            ff_dim=MODEL_DIM * 4,  # Típicamente 4x model_dim\n",
    "            dropout=0.1\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        history = run_training_transformer(\n",
    "            model, \n",
    "            tr_loader, \n",
    "            te_loader,\n",
    "            epochs=EPOCHS, \n",
    "            lr=LR\n",
    "        )\n",
    "\n",
    "        # Guardar resultados\n",
    "        results_transformer.append({\n",
    "            'extractor': fname,\n",
    "            'model': 'Transformer',\n",
    "            'normalization': norm,\n",
    "            'accuracy': history['test_acc'][-1],\n",
    "            'f1_macro': history['test_f1'][-1]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "df_results = pd.DataFrame(results_transformer)\n",
    "df_results.to_csv('transformer_results.csv', index=False)\n",
    "print(\"\\n✅ Resultados Transformer guardados en transformer_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc05cb",
   "metadata": {},
   "source": [
    "## Modelo TCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bb454",
   "metadata": {},
   "source": [
    "## Entrenamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6e756",
   "metadata": {},
   "source": [
    "## Métricas posibles a usar\n",
    "Precisión global (Accuracy)\n",
    "\n",
    "F₁‐score macro\n",
    "Matriz de confusión\n",
    "\n",
    "Precisión (Precision) por clase\n",
    "\n",
    "Exhaustividad (Recall) por clase\n",
    "\n",
    "Balanced accuracy (accuracy balanceada)\n",
    "\n",
    "Matthew’s Correlation Coefficient (MCC)\n",
    "\n",
    "Curva ROC y AUC multiclass (one-vs-rest)\n",
    "\n",
    "Log-Loss (Cross-Entropy Loss)\n",
    "\n",
    "Brier Score\n",
    "\n",
    "Cohen’s Kappa\n",
    "\n",
    "Top-k accuracy (por ejemplo Top-2)\n",
    "\n",
    "Time-to-decision (número medio de frames o ms antes del golpeo en que la predicción es estable)\n",
    "\n",
    "Área bajo la curva Accuracy vs. Earliness\n",
    "\n",
    "Tiempo de inferencia por muestra (latencia)\n",
    "\n",
    "Número de parámetros / FLOPS / uso de memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c0c6a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "056bc2e9",
   "metadata": {},
   "source": [
    "\n",
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9554f65",
   "metadata": {},
   "source": [
    "Cuando tienes una secuencia de embeddings de forma (T,D) —es decir, \n",
    "T instantes en el tiempo y D características en cada uno— el pooling temporal la transforma en un único vector de dimensión D. Para cada característica j (j=1,…,D) se calcula bien la media de sus T valores (mean pooling) o 4bien su máximo (max pooling) a lo largo de toda la secuencia. De este modo, cada componente del vector resultante sintetiza la evolución completa de esa característica en el intervalo temporal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184e678",
   "metadata": {},
   "source": [
    "Para el MLP, la función prepare_mlp_data agrupa cada secuencia (T,D) y aplica pooling (media o máximo) para convertirla en un vector plano de dimensión (D,), que luego normaliza y envía al modelo como input (batch, D). Por otro lado, SequenceDataset junto a collate_sequences conservan la secuencia completa de embeddings (T, D), la rellenan con ceros hasta T_max y devuelven también un tensor de longitudes reales, de modo que tu LSTM y Transformer reciben entradas de forma (batch, T_max, D) y usan esas longitudes para ignorar el padding durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) Dataset y Collate para secuencias\n",
    "# ------------------------------------------------\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        feature_cols = [c for c in df.columns if c.startswith('feat_')]\n",
    "        self.samples = []\n",
    "        for vid, grp in df.groupby('video_ID'):\n",
    "            arr = torch.from_numpy(grp[feature_cols].values).float()  # (T,D)\n",
    "            label = int(grp['shoot_zone'].iloc[0])\n",
    "            self.samples.append((arr, label))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "def collate_sequences(batch):\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([s.size(0) for s in seqs], dtype=torch.long)\n",
    "    padded = pad_sequence(seqs, batch_first=True)  # (B, T_max, D)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded, lengths, labels\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) Funciones de preparación de datos\n",
    "# ------------------------------------------------\n",
    "def prepare_mlp_data(df, pooling='mean', test_size=0.2):\n",
    "    # Extrae vectores (T,D) → pooling → (D,) y etiquetas\n",
    "    feature_cols = [c for c in df.columns if c.startswith('feat_')]\n",
    "    seqs, labs = [], []\n",
    "    for vid, grp in df.groupby('video_ID'):\n",
    "        arr = grp[feature_cols].values  # (T,D)\n",
    "        vec = arr.mean(axis=0) if pooling=='mean' else arr.max(axis=0)\n",
    "        seqs.append(vec)\n",
    "        labs.append(int(grp['shoot_zone'].iloc[0]))\n",
    "    X = np.stack(seqs).astype(np.float32)\n",
    "    y = np.array(labs, dtype=np.int64)\n",
    "    # Normalizamos MinMax\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    # Split\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size,\n",
    "                                          stratify=y, random_state=42)\n",
    "    tr_ds = TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(ytr))\n",
    "    te_ds = TensorDataset(torch.from_numpy(Xte), torch.from_numpy(yte))\n",
    "    return tr_ds, te_ds\n",
    "\n",
    "def prepare_seq_data(df, test_size=0.2):\n",
    "    # Dataset completo\n",
    "    full = SequenceDataset(df)\n",
    "    labels = [full[i][1] for i in range(len(full))]\n",
    "    idx_tr, idx_te = train_test_split(list(range(len(full))),\n",
    "                                      test_size=test_size,\n",
    "                                      stratify=labels,\n",
    "                                      random_state=42)\n",
    "    tr_ds = torch.utils.data.Subset(full, idx_tr)\n",
    "    te_ds = torch.utils.data.Subset(full, idx_te)\n",
    "    return tr_ds, te_ds\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) Bucle de entrenamiento + evaluación\n",
    "# ------------------------------------------------\n",
    "def run_training(model, train_loader, test_loader, epochs=20, lr=1e-3):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    history = {'train_loss':[], 'test_acc':[], 'test_f1':[]}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # --- entrenamiento\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            if isinstance(model, MLPClassifier):\n",
    "                xb, yb = batch\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                out = model(xb)\n",
    "            else:\n",
    "                xb, lengths, yb = batch\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                lengths = lengths.to(DEVICE)\n",
    "                out = model(xb, lengths)\n",
    "            loss = loss_fn(out, yb)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "        history['train_loss'].append(total_loss / len(train_loader.dataset))\n",
    "\n",
    "        # --- evaluación\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                if isinstance(model, MLPClassifier):\n",
    "                    xb, yb = batch\n",
    "                    xb = xb.to(DEVICE)\n",
    "                    preds = model(xb).argmax(dim=1).cpu().numpy()\n",
    "                else:\n",
    "                    xb, lengths, yb = batch\n",
    "                    xb = xb.to(DEVICE); lengths = lengths.to(DEVICE)\n",
    "                    preds = model(xb, lengths).argmax(dim=1).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(yb.numpy())\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        history['test_acc'].append(acc)\n",
    "        history['test_f1'].append(f1)\n",
    "\n",
    "        print(f\"Ep{ep:02d} | loss {history['train_loss'][-1]:.4f} | \"\n",
    "              f\"acc {acc:.4f} | f1_macro {f1:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d624c",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "data_dir = \"Gait_Embeddings_good/\"\n",
    "extractors = [fname for fname in os.listdir(data_dir)]\n",
    "pools      = ['mean', 'max']\n",
    "norms      = ['minmax', 'l2']\n",
    "classifiers = {\n",
    "    'MLP':    MLPClassifier,\n",
    "    #'LSTM':   LSTMClassifier,\n",
    "    #'Transf': TransformerClassifier,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for csv_file in extractors:\n",
    "    df = pd.read_csv(f'Gait_Embeddings_good/{csv_file}')\n",
    "        \n",
    "    # ————————————————————————\n",
    "    # 1) MLP (usa pooling + norm)\n",
    "    # ————————————————————————\n",
    "    for pool in pools:\n",
    "        for norm in norms:\n",
    "            tr_ds, te_ds = prepare_mlp_data(df, pooling=pool)\n",
    "            # crea loaders\n",
    "            tr_loader = DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "            te_loader = DataLoader(te_ds, batch_size=32)\n",
    "\n",
    "            model = MLPClassifier(input_dim=tr_ds[0][0].shape[0]).to(DEVICE)\n",
    "            history = run_training(model, tr_loader, te_loader, epochs=20, lr=1e-3)\n",
    "\n",
    "            # extraer métricas finales\n",
    "            final_acc = history['test_acc'][-1]\n",
    "            final_f1  = history['test_f1'][-1]\n",
    "\n",
    "            results.append({\n",
    "                'extractor':    csv_file,\n",
    "                'model':        'MLP',\n",
    "                'pooling':      pool,\n",
    "                'normalization':norm,\n",
    "                'accuracy':     final_acc,\n",
    "                'f1_macro':     final_f1\n",
    "            })\n",
    "\"\"\"\n",
    "    # ————————————————————————\n",
    "    # 2) LSTM y Transformer (sin pooling/norm)\n",
    "    # ————————————————————————\n",
    "    tr_seq, te_seq = prepare_seq_data(df)\n",
    "    for name, Cls in [('LSTM', LSTMClassifier), ('Transf', TransformerClassifier)]:\n",
    "        tr_loader = DataLoader(tr_seq, batch_size=16, shuffle=True, collate_fn=collate_sequences)\n",
    "        te_loader = DataLoader(te_seq, batch_size=16, collate_fn=collate_sequences)\n",
    "\n",
    "        model = Cls(input_dim=df.filter(like='feat_').shape[1]).to(DEVICE)\n",
    "        history = run_training(model, tr_loader, te_loader, epochs=20, lr=1e-3)\n",
    "\n",
    "        final_acc = history['test_acc'][-1]\n",
    "        final_f1  = history['test_f1'][-1]\n",
    "\n",
    "        results.append({\n",
    "            'extractor':     csv_file,\n",
    "            'model':         name,\n",
    "            'pooling':       None,\n",
    "            'normalization': None,\n",
    "            'accuracy':      final_acc,\n",
    "            'f1_macro':      final_f1\n",
    "        })\n",
    "\"\"\"\n",
    "# ————————————————————————\n",
    "# Volcar a CSV\n",
    "# ————————————————————————\n",
    "df_res = pd.DataFrame(results)\n",
    "df_res.to_csv('resultados_penaltis.csv', index=False)\n",
    "print(\"✅ Resultados guardados en resultados_penaltis.csv\")\n",
    "print(df_res)\n",
    "\n",
    "\n",
    "# Guadar el modelo final\n",
    "#torch.save(model.state_dict(), 'model_final.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4596a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
