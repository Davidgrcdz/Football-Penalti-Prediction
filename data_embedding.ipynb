{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083962d8",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Cada modelo tiene todos los vídeos y la etiqueta en la columna **shoot_zone**,  donde lanzamiento a la derecha es 0, al centro es 1 y a la izquierda  es 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcdd711",
   "metadata": {},
   "source": [
    "| Fichero                      | Modelo          | Comentarios sobre “\\_SUFIJO”                              |\n",
    "| ---------------------------- | --------------- | --------------------------------------------------------- |\n",
    "| **baseline\\_CASIAB.csv**     | Baseline        | Versión “base” (p.ej. GEINet simple) entrenada en CASIA-B |\n",
    "| **baseline\\_OUMVLP.csv**     | Baseline        | Igual que el anterior, pero pre-entrenado en OU-MVLP      |\n",
    "| **gaitgl.csv**               | GaitGL          | GaitGL estándar (dataset por defecto, p.ej. CASIA-B)      |\n",
    "| **gaitgl\\_OUMVLP.csv**       | GaitGL          | Pre-entrenado en OU-MVLP                                  |\n",
    "| **gaitgl\\_GREW\\.csv**        | GaitGL          | Pre-entrenado en GREW                                     |\n",
    "| **gaitgl\\_GREW\\_BNNeck.csv** | GaitGL + BNNeck | Mismo que el anterior, con cuello de batch-norm extra     |\n",
    "| **gaitpart.csv**             | GaitPart        | GaitPart estándar                                         |\n",
    "| **gaitpart\\_OUMVLP.csv**     | GaitPart        | Pre-entrenado en OU-MVLP                                  |\n",
    "| **gaitpart\\_GREW\\.csv**      | GaitPart        | Pre-entrenado en GREW                                     |\n",
    "| **gaitset.csv**              | GaitSet         | GaitSet estándar                                          |\n",
    "| **gaitset\\_OUMVLP.csv**      | GaitSet         | Pre-entrenado en OU-MVLP                                  |\n",
    "| **gaitset\\_GREW\\.csv**       | GaitSet         | Pre-entrenado en GREW                                     |\n",
    "| **gln\\_phase1.csv**          | GLN (fase 1)    | Primer bloque/fase de extracción del modelo “GLN”         |\n",
    "| **gln\\_phase2.csv**          | GLN (fase 2)    | Fase de refinamiento o bloque final del mismo “GLN”       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Directorio con los CSV\n",
    "data_dir = \"Gait_Embeddings_good/\"\n",
    "\n",
    "# 2. Listar sólo los archivos .csv\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "print(\"Archivos encontrados:\", csv_files)\n",
    "\n",
    "# 3. Leer cada CSV en un DataFrame de pandas\n",
    "dfs = {}\n",
    "for fname in csv_files:\n",
    "    path = os.path.join(data_dir, fname)\n",
    "    dfs[fname] = pd.read_csv(path)\n",
    "\n",
    "# 4. Explorar cada DataFrame\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Shape:\", df.shape)                  # filas × columnas\n",
    "    print(\"Columnas:\", df.columns.tolist())    # lista de nombres\n",
    "    print(\"Primeras 5 filas:\")\n",
    "    print(df.head().to_string(index=False))    # muestra las primeras filas\n",
    "\n",
    "    # Opcional: ver tipo de datos y memoria\n",
    "    print(\"\\nInfo:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nDescripción estadística de columnas numéricas:\")\n",
    "    print(df.describe().T)  # transpuesta para leer mejor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f291b9",
   "metadata": {},
   "source": [
    "## Metodología de desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf249d",
   "metadata": {},
   "source": [
    "1️⃣ datos → 2️⃣ preprocesado y normalización → 3️⃣ split → 4️⃣ Dataset/DataLoader (+ collate) → 5️⃣ modelo → 6️⃣ entrenamiento (función de pérdida y optimizador) → 7️⃣ evaluación → 8️⃣ ajuste → 9️⃣ guardado.\n",
    "\n",
    "- Entender y explorar los datos\n",
    "\n",
    "- Inspecciona las columnas, tipos de variables, balance de clases, valores faltantes y rangos.\n",
    "\n",
    "- Visualiza distribuciones y posibles outliers.\n",
    "\n",
    "- Limpieza y preprocesado\n",
    "\n",
    "- Trata valores faltantes (imputación o eliminación).\n",
    "\n",
    "- Normalización / escalado\n",
    "\n",
    "- Aplica Min–Max o Z-score (standarización) para que todas las características queden en un rango controlado y evites que alguna domine el entrenamiento.\n",
    "\n",
    "- En embeddings, a veces se usa L₂-norm para cada vector si quieres que tengan norma unidad.\n",
    "\n",
    "- Dividir en train / validation / test\n",
    "\n",
    "- Reserva al menos un 10–20 % para test “final”.\n",
    "\n",
    "- Dentro del train crea validación (p. ej. 80/20 o K-fold) para ajustar hiperparámetros sin tocar el test.\n",
    "\n",
    "- Definir Dataset y DataLoader para construir batches\n",
    "\n",
    "- Definir el modelo\n",
    "\n",
    "- Elegir función de pérdida y optimizador\n",
    "\n",
    "- Bucle de entrenamiento\n",
    "\n",
    "- Ajuste de hiperparámetros\n",
    "\n",
    "- Guardado y almacenado de los pesos con torch.save(model.state_dict(), 'modelo.pt').\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b5b8c0",
   "metadata": {},
   "source": [
    "## Definición de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95440a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "\n",
    "# 1) MLP sencillo con pooling previo\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        input_dim: dimensión D del embedding tras pooling\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 3)  # 3 clases: derecha, centro, izquierda\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, D)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "# 2) LSTM (o BiLSTM) con salida densa\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        bidirectional=False,\n",
    "        dropout=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_dim: dimensión D del embedding sin pooling (secuencia)\n",
    "        hidden_dim: tamaño de la capa oculta LSTM\n",
    "        num_layers: número de capas LSTM\n",
    "        bidirectional: si usar BiLSTM\n",
    "        dropout: dropout entre capas LSTM (si num_layers > 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        final_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(final_dim, 3)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        x: tensor FloatTensor de forma (B, T, D)\n",
    "        lengths: LongTensor con longitudes reales de cada secuencia (B,)\n",
    "        \"\"\"\n",
    "        # Empaquetar secuencias variables\n",
    "        packed = pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, (h_n, _) = self.lstm(packed)\n",
    "        # h_n tiene forma (num_layers * num_directions, B, hidden_dim)\n",
    "        # Extraemos la última capa de la última dirección\n",
    "        if self.lstm.bidirectional:\n",
    "            # Dos direcciones: concatenar capa final forward y backward\n",
    "            h_forward = h_n[-2]  # última capa forward\n",
    "            h_backward = h_n[-1]  # última capa backward\n",
    "            h_final = torch.cat([h_forward, h_backward], dim=1)\n",
    "        else:\n",
    "            # Solo una dirección\n",
    "            h_final = h_n[-1]\n",
    "        return self.fc(h_final)\n",
    "\n",
    "\n",
    "\n",
    "# 3) Transformer con atención temporal\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        model_dim=256,\n",
    "        n_heads=4,\n",
    "        num_layers=2,\n",
    "        ff_dim=512,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_dim: dimensión D del embedding sin pooling (secuencia)\n",
    "        model_dim: dimensión interna del transformer\n",
    "        n_heads: número de cabezas de atención\n",
    "        num_layers: número de capas Encoder\n",
    "        ff_dim: dimensión del feed-forward\n",
    "        dropout: dropout en capas Encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Proyección de la dimensión de entrada al espacio model_dim\n",
    "        self.token_proj = nn.Linear(input_dim, model_dim)\n",
    "        # Capa TransformerEncoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Clasificador al token de posición 0 (CLS) o media de salidas\n",
    "        self.classifier = nn.Linear(model_dim, 3)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        \"\"\"\n",
    "        x: FloatTensor (B, T, D)\n",
    "        lengths: LongTensor (B,) opcional para masking\n",
    "        \"\"\"\n",
    "        # x → proyección\n",
    "        x = self.token_proj(x)  # (B, T, model_dim)\n",
    "\n",
    "        # Generar máscara de padding si lengths dado\n",
    "        if lengths is not None:\n",
    "            max_len = x.size(1)\n",
    "            mask = torch.arange(max_len, device=lengths.device) \\\n",
    "                   .unsqueeze(0) >= lengths.unsqueeze(1)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        # TransformerEncoder\n",
    "        out = self.transformer(x, src_key_padding_mask=mask)  # (B, T, model_dim)\n",
    "\n",
    "        # Agregado temporal: tomamos token 0 como representativo\n",
    "        cls_token = out[:, 0, :]  # (B, model_dim)\n",
    "        return self.classifier(cls_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586aa6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "004df432",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Preparación de datos, split y bucle de entrenamiento: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c64bfa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def prepare_mlp_data(df, pooling, norm, test_size=0.2):\n",
    "    # 1) Pooling temporal\n",
    "    feat_cols = [c for c in df.columns if c.startswith('feat_')]\n",
    "    seqs, labs = [], []\n",
    "    for vid, grp in df.groupby('video_ID'):\n",
    "        arr = grp[feat_cols].values.astype(np.float32)  # (T, D)\n",
    "        vec = arr.mean(axis=0) if pooling=='mean' else arr.max(axis=0)\n",
    "        seqs.append(vec)\n",
    "        labs.append(int(grp['shoot_zone'].iloc[0]))\n",
    "    X = np.stack(seqs)  # (N, D)\n",
    "    y = np.array(labs, dtype=np.int64)\n",
    "\n",
    "    # 2) Normalización\n",
    "    if norm == 'minmax':\n",
    "        X = MinMaxScaler().fit_transform(X)\n",
    "    elif norm == 'L2':  # 'l2'\n",
    "        X = normalize(X, norm='l2')\n",
    "\n",
    "    # 3) Split estratificado\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # 4) TensorDatasets\n",
    "    tr_ds = TensorDataset(torch.from_numpy(Xtr).float(),\n",
    "                          torch.from_numpy(ytr))\n",
    "    te_ds = TensorDataset(torch.from_numpy(Xte).float(),\n",
    "                          torch.from_numpy(yte))\n",
    "    return tr_ds, te_ds\n",
    "\n",
    "def run_training(model, train_loader, test_loader, epochs=20, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn   = nn.CrossEntropyLoss()\n",
    "    history   = {'train_loss':[], 'test_acc':[], 'test_f1':[]}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # entrenamiento\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "        history['train_loss'].append(total_loss / len(train_loader.dataset))\n",
    "\n",
    "        # evaluación\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                preds = model(xb).argmax(dim=1).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(yb.numpy())\n",
    "        all_preds  = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1  = f1_score(all_labels, all_preds, average='macro')\n",
    "        history['test_acc'].append(acc)\n",
    "        history['test_f1'].append(f1)\n",
    "\n",
    "        print(f\"Ep{ep:02d} | loss {history['train_loss'][-1]:.4f} \"\n",
    "              f\"| acc {acc:.4f} | f1_macro {f1:.4f}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d20515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mlp.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DATA_DIR = \"Gait_Embeddings_good\"\n",
    "POOLS    = ['mean', 'max']\n",
    "NORMS    = ['minmax', 'L2']\n",
    "EPOCHS   = 20\n",
    "BATCH    = 32\n",
    "\n",
    "results = []\n",
    "\n",
    "for fname in os.listdir(DATA_DIR):\n",
    "    if not fname.endswith('.csv'):\n",
    "        continue\n",
    "    print(f\"\\n--- Entrenando MLP con {fname} ---\")\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, fname))\n",
    "\n",
    "    for pool in POOLS:\n",
    "        for norm in NORMS:\n",
    "            tr_ds, te_ds = prepare_mlp_data(df, pooling=pool, norm=norm, test_size=0.2\n",
    "            )\n",
    "            tr_loader = DataLoader(tr_ds, batch_size=BATCH, shuffle=True)\n",
    "            te_loader = DataLoader(te_ds, batch_size=BATCH)\n",
    "\n",
    "            model = MLPClassifier(tr_ds[0][0].shape[0]).to(DEVICE)\n",
    "            history = run_training(\n",
    "                model, tr_loader, te_loader,\n",
    "                epochs=EPOCHS, lr=1e-3\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                'extractor':     fname,\n",
    "                'model':         'MLP',\n",
    "                'pooling':       pool,\n",
    "                'normalization': norm,\n",
    "                'accuracy':      history['test_acc'][-1],\n",
    "                'f1_macro':      history['test_f1'][-1]\n",
    "            })\n",
    "\n",
    "# Guardar resultados finales\n",
    "import pandas as pd\n",
    "df_res = pd.DataFrame(results)\n",
    "df_res.to_csv('mlp_penalty_results.csv', index=False)\n",
    "print(\"\\n✅ Resultados guardados en mlp_penalty_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc2d0d",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Preparación de datos, split y bucle de entrenamiento: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ec41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_sequences(batch):\n",
    "    \"\"\"\n",
    "    Recibe una lista de (tensor_seq, label).\n",
    "    Devuelve:\n",
    "      - padded: tensor (B, T_max, D)\n",
    "      - lengths: tensor (B,)\n",
    "      - labels: tensor (B,)\n",
    "    \"\"\"\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([s.size(0) for s in seqs], dtype=torch.long)\n",
    "    padded = pad_sequence(seqs, batch_first=True)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded, lengths, labels\n",
    "\n",
    "def prepare_seq_data(df, norm='minmax', test_size=0.2):\n",
    "    \"\"\"\n",
    "    df: DataFrame con columnas feat_0…feat_D-1, video_ID y shoot_zone\n",
    "    norm: 'minmax' o 'l2'\n",
    "    Devuelve dos listas de muestras (tensor_seq, label) para train y test.\n",
    "    \"\"\"\n",
    "    # 1) Extraer todas las secuencias y etiquetas\n",
    "    feat_cols = [c for c in df.columns if c.startswith('feat_')]\n",
    "    seqs, labs = [], []\n",
    "    for vid, grp in df.groupby('video_ID'):\n",
    "        arr = grp[feat_cols].values.astype(np.float32)  # (T, D)\n",
    "        seqs.append(arr)\n",
    "        labs.append(int(grp['shoot_zone'].iloc[0]))\n",
    "\n",
    "    # 2) Normalizar\n",
    "    if norm == 'minmax':\n",
    "        all_frames = np.vstack(seqs)\n",
    "        scaler = MinMaxScaler().fit(all_frames)\n",
    "        seqs = [scaler.transform(s) for s in seqs]\n",
    "    else:  # L2-norm por frame\n",
    "        seqs = [s / np.linalg.norm(s, axis=1, keepdims=True) for s in seqs]\n",
    "\n",
    "    # 3) Split estratificado\n",
    "    idx = list(range(len(seqs)))\n",
    "    idx_tr, idx_te = train_test_split(\n",
    "        idx, test_size=test_size, stratify=labs, random_state=42\n",
    "    )\n",
    "\n",
    "    # 4) Convertir a listas de tuplas (tensor_seq, label)\n",
    "    train_list = [(torch.from_numpy(seqs[i]), labs[i]) for i in idx_tr]\n",
    "    test_list  = [(torch.from_numpy(seqs[i]), labs[i]) for i in idx_te]\n",
    "\n",
    "    return train_list, test_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a897a0",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41784538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Métodos ya definidos en los chunks anteriores:\n",
    "# - LSTMClassifier\n",
    "# - collate_sequences\n",
    "# - prepare_seq_data\n",
    "# - run_training (tu función genérica de entrenamiento/eval)\n",
    "\n",
    "DATA_DIR = \"Gait_Embeddings_good\"\n",
    "NORMS    = ['minmax', 'l2']\n",
    "EPOCHS   = 20\n",
    "LR       = 1e-3\n",
    "BATCH    = 16\n",
    "\n",
    "results = []\n",
    "\n",
    "for fname in os.listdir(DATA_DIR):\n",
    "    if not fname.endswith('.csv'):\n",
    "        continue\n",
    "    print(f\"\\n--- Entrenando LSTM con {fname} ---\")\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, fname))\n",
    "\n",
    "    for norm in NORMS:\n",
    "        # Prepara los datos secuenciales\n",
    "        train_list, test_list = prepare_seq_data(df, norm=norm)\n",
    "        tr_loader = DataLoader(train_list, batch_size=BATCH,\n",
    "                               shuffle=True, collate_fn=collate_sequences)\n",
    "        te_loader = DataLoader(test_list, batch_size=BATCH,\n",
    "                               collate_fn=collate_sequences)\n",
    "\n",
    "        # Instanciar y entrenar\n",
    "        model = LSTMClassifier(input_dim=df.filter(like='feat_').shape[1])\\\n",
    "                    .to(DEVICE)\n",
    "        history = run_training(model, tr_loader, te_loader,\n",
    "                               epochs=EPOCHS, lr=LR)\n",
    "\n",
    "        # Almacenar métricas finales\n",
    "        results.append({\n",
    "            'extractor':     fname,\n",
    "            'model':         'LSTM',\n",
    "            'pooling':       None,\n",
    "            'normalization': norm,\n",
    "            'accuracy':      history['test_acc'][-1],\n",
    "            'f1_macro':      history['test_f1'][-1]\n",
    "        })\n",
    "\n",
    "# Guardar todos los resultados en CSV\n",
    "df_res = pd.DataFrame(results)\n",
    "df_res.to_csv('lstm_penalty_results.csv', index=False)\n",
    "print(\"\\n✅ Resultados LSTM guardados en lstm_penalty_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eef620",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Preparación de datos, split y bucle de entrenamiento: BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68132036",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c355e",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Preparación de datos, split y bucle de entrenamiento: Transformer\n",
    "Con atención atemporal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e5359",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6e756",
   "metadata": {},
   "source": [
    "## Métricas posibles a usar\n",
    "Precisión global (Accuracy)\n",
    "\n",
    "F₁‐score macro\n",
    "Matriz de confusión\n",
    "\n",
    "Precisión (Precision) por clase\n",
    "\n",
    "Exhaustividad (Recall) por clase\n",
    "\n",
    "Balanced accuracy (accuracy balanceada)\n",
    "\n",
    "Matthew’s Correlation Coefficient (MCC)\n",
    "\n",
    "Curva ROC y AUC multiclass (one-vs-rest)\n",
    "\n",
    "Log-Loss (Cross-Entropy Loss)\n",
    "\n",
    "Brier Score\n",
    "\n",
    "Cohen’s Kappa\n",
    "\n",
    "Top-k accuracy (por ejemplo Top-2)\n",
    "\n",
    "Time-to-decision (número medio de frames o ms antes del golpeo en que la predicción es estable)\n",
    "\n",
    "Área bajo la curva Accuracy vs. Earliness\n",
    "\n",
    "Tiempo de inferencia por muestra (latencia)\n",
    "\n",
    "Número de parámetros / FLOPS / uso de memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c0c6a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "056bc2e9",
   "metadata": {},
   "source": [
    "\n",
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9554f65",
   "metadata": {},
   "source": [
    "Cuando tienes una secuencia de embeddings de forma (T,D) —es decir, \n",
    "T instantes en el tiempo y D características en cada uno— el pooling temporal la transforma en un único vector de dimensión D. Para cada característica j (j=1,…,D) se calcula bien la media de sus T valores (mean pooling) o 4bien su máximo (max pooling) a lo largo de toda la secuencia. De este modo, cada componente del vector resultante sintetiza la evolución completa de esa característica en el intervalo temporal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184e678",
   "metadata": {},
   "source": [
    "Para el MLP, la función prepare_mlp_data agrupa cada secuencia (T,D) y aplica pooling (media o máximo) para convertirla en un vector plano de dimensión (D,), que luego normaliza y envía al modelo como input (batch, D). Por otro lado, SequenceDataset junto a collate_sequences conservan la secuencia completa de embeddings (T, D), la rellenan con ceros hasta T_max y devuelven también un tensor de longitudes reales, de modo que tu LSTM y Transformer reciben entradas de forma (batch, T_max, D) y usan esas longitudes para ignorar el padding durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) Dataset y Collate para secuencias\n",
    "# ------------------------------------------------\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        feature_cols = [c for c in df.columns if c.startswith('feat_')]\n",
    "        self.samples = []\n",
    "        for vid, grp in df.groupby('video_ID'):\n",
    "            arr = torch.from_numpy(grp[feature_cols].values).float()  # (T,D)\n",
    "            label = int(grp['shoot_zone'].iloc[0])\n",
    "            self.samples.append((arr, label))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "def collate_sequences(batch):\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([s.size(0) for s in seqs], dtype=torch.long)\n",
    "    padded = pad_sequence(seqs, batch_first=True)  # (B, T_max, D)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded, lengths, labels\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) Funciones de preparación de datos\n",
    "# ------------------------------------------------\n",
    "def prepare_mlp_data(df, pooling='mean', test_size=0.2):\n",
    "    # Extrae vectores (T,D) → pooling → (D,) y etiquetas\n",
    "    feature_cols = [c for c in df.columns if c.startswith('feat_')]\n",
    "    seqs, labs = [], []\n",
    "    for vid, grp in df.groupby('video_ID'):\n",
    "        arr = grp[feature_cols].values  # (T,D)\n",
    "        vec = arr.mean(axis=0) if pooling=='mean' else arr.max(axis=0)\n",
    "        seqs.append(vec)\n",
    "        labs.append(int(grp['shoot_zone'].iloc[0]))\n",
    "    X = np.stack(seqs).astype(np.float32)\n",
    "    y = np.array(labs, dtype=np.int64)\n",
    "    # Normalizamos MinMax\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    # Split\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size,\n",
    "                                          stratify=y, random_state=42)\n",
    "    tr_ds = TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(ytr))\n",
    "    te_ds = TensorDataset(torch.from_numpy(Xte), torch.from_numpy(yte))\n",
    "    return tr_ds, te_ds\n",
    "\n",
    "def prepare_seq_data(df, test_size=0.2):\n",
    "    # Dataset completo\n",
    "    full = SequenceDataset(df)\n",
    "    labels = [full[i][1] for i in range(len(full))]\n",
    "    idx_tr, idx_te = train_test_split(list(range(len(full))),\n",
    "                                      test_size=test_size,\n",
    "                                      stratify=labels,\n",
    "                                      random_state=42)\n",
    "    tr_ds = torch.utils.data.Subset(full, idx_tr)\n",
    "    te_ds = torch.utils.data.Subset(full, idx_te)\n",
    "    return tr_ds, te_ds\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) Bucle de entrenamiento + evaluación\n",
    "# ------------------------------------------------\n",
    "def run_training(model, train_loader, test_loader, epochs=20, lr=1e-3):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    history = {'train_loss':[], 'test_acc':[], 'test_f1':[]}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # --- entrenamiento\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            if isinstance(model, MLPClassifier):\n",
    "                xb, yb = batch\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                out = model(xb)\n",
    "            else:\n",
    "                xb, lengths, yb = batch\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                lengths = lengths.to(DEVICE)\n",
    "                out = model(xb, lengths)\n",
    "            loss = loss_fn(out, yb)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "        history['train_loss'].append(total_loss / len(train_loader.dataset))\n",
    "\n",
    "        # --- evaluación\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                if isinstance(model, MLPClassifier):\n",
    "                    xb, yb = batch\n",
    "                    xb = xb.to(DEVICE)\n",
    "                    preds = model(xb).argmax(dim=1).cpu().numpy()\n",
    "                else:\n",
    "                    xb, lengths, yb = batch\n",
    "                    xb = xb.to(DEVICE); lengths = lengths.to(DEVICE)\n",
    "                    preds = model(xb, lengths).argmax(dim=1).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(yb.numpy())\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        history['test_acc'].append(acc)\n",
    "        history['test_f1'].append(f1)\n",
    "\n",
    "        print(f\"Ep{ep:02d} | loss {history['train_loss'][-1]:.4f} | \"\n",
    "              f\"acc {acc:.4f} | f1_macro {f1:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d624c",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "data_dir = \"Gait_Embeddings_good/\"\n",
    "extractors = [fname for fname in os.listdir(data_dir)]\n",
    "pools      = ['mean', 'max']\n",
    "norms      = ['minmax', 'l2']\n",
    "classifiers = {\n",
    "    'MLP':    MLPClassifier,\n",
    "    #'LSTM':   LSTMClassifier,\n",
    "    #'Transf': TransformerClassifier,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for csv_file in extractors:\n",
    "    df = pd.read_csv(f'Gait_Embeddings_good/{csv_file}')\n",
    "        \n",
    "    # ————————————————————————\n",
    "    # 1) MLP (usa pooling + norm)\n",
    "    # ————————————————————————\n",
    "    for pool in pools:\n",
    "        for norm in norms:\n",
    "            tr_ds, te_ds = prepare_mlp_data(df, pooling=pool)\n",
    "            # crea loaders\n",
    "            tr_loader = DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "            te_loader = DataLoader(te_ds, batch_size=32)\n",
    "\n",
    "            model = MLPClassifier(input_dim=tr_ds[0][0].shape[0]).to(DEVICE)\n",
    "            history = run_training(model, tr_loader, te_loader, epochs=20, lr=1e-3)\n",
    "\n",
    "            # extraer métricas finales\n",
    "            final_acc = history['test_acc'][-1]\n",
    "            final_f1  = history['test_f1'][-1]\n",
    "\n",
    "            results.append({\n",
    "                'extractor':    csv_file,\n",
    "                'model':        'MLP',\n",
    "                'pooling':      pool,\n",
    "                'normalization':norm,\n",
    "                'accuracy':     final_acc,\n",
    "                'f1_macro':     final_f1\n",
    "            })\n",
    "\"\"\"\n",
    "    # ————————————————————————\n",
    "    # 2) LSTM y Transformer (sin pooling/norm)\n",
    "    # ————————————————————————\n",
    "    tr_seq, te_seq = prepare_seq_data(df)\n",
    "    for name, Cls in [('LSTM', LSTMClassifier), ('Transf', TransformerClassifier)]:\n",
    "        tr_loader = DataLoader(tr_seq, batch_size=16, shuffle=True, collate_fn=collate_sequences)\n",
    "        te_loader = DataLoader(te_seq, batch_size=16, collate_fn=collate_sequences)\n",
    "\n",
    "        model = Cls(input_dim=df.filter(like='feat_').shape[1]).to(DEVICE)\n",
    "        history = run_training(model, tr_loader, te_loader, epochs=20, lr=1e-3)\n",
    "\n",
    "        final_acc = history['test_acc'][-1]\n",
    "        final_f1  = history['test_f1'][-1]\n",
    "\n",
    "        results.append({\n",
    "            'extractor':     csv_file,\n",
    "            'model':         name,\n",
    "            'pooling':       None,\n",
    "            'normalization': None,\n",
    "            'accuracy':      final_acc,\n",
    "            'f1_macro':      final_f1\n",
    "        })\n",
    "\"\"\"\n",
    "# ————————————————————————\n",
    "# Volcar a CSV\n",
    "# ————————————————————————\n",
    "df_res = pd.DataFrame(results)\n",
    "df_res.to_csv('resultados_penaltis.csv', index=False)\n",
    "print(\"✅ Resultados guardados en resultados_penaltis.csv\")\n",
    "print(df_res)\n",
    "\n",
    "\n",
    "# Guadar el modelo final\n",
    "#torch.save(model.state_dict(), 'model_final.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4596a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
